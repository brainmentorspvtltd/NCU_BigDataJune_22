{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171ee6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919a2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56fe85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Example_2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff55ba77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-69NKCP74.mshome.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Example_2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2a5cc0d2eb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311f627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile('pyspark_intro.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e94a5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28de159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = rdd.flatMap(lambda x : x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a97c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', '-', 'Open', 'Source', 'Analytical', 'Processing', 'Engine', '-', 'Immutable', '-', 'Fault', 'tolerant', '-', 'Cache', '&', 'Persistence', '-', 'In-Memory', '-', 'Real', 'time', 'processing', 'using', 'Streaming', 'and', 'Kafka', 'PySpark', '-', 'Spark', 'Library', 'written', 'in', 'Python', '', 'Stream', '-', 'continuous', 'flow', 'of', 'data', '', 'Buffering', '', '', '', '-', 'chunk', 'of', 'data', 'that', 'is', 'coming', 'from', 'somewhere', 'Streaming', '', '', '-', 'continuous', 'flow', 'of', 'data', 'coming', 'from', 'somewhere']\n"
     ]
    }
   ],
   "source": [
    "print(rdd_2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a185774",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_3 = rdd_2.map(lambda x : (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaacf6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spark', 1), ('-', 1), ('Open', 1), ('Source', 1), ('Analytical', 1), ('Processing', 1), ('Engine', 1), ('-', 1), ('Immutable', 1), ('-', 1), ('Fault', 1), ('tolerant', 1), ('-', 1), ('Cache', 1), ('&', 1), ('Persistence', 1), ('-', 1), ('In-Memory', 1), ('-', 1), ('Real', 1), ('time', 1), ('processing', 1), ('using', 1), ('Streaming', 1), ('and', 1), ('Kafka', 1), ('PySpark', 1), ('-', 1), ('Spark', 1), ('Library', 1), ('written', 1), ('in', 1), ('Python', 1), ('', 1), ('Stream', 1), ('-', 1), ('continuous', 1), ('flow', 1), ('of', 1), ('data', 1), ('', 1), ('Buffering', 1), ('', 1), ('', 1), ('', 1), ('-', 1), ('chunk', 1), ('of', 1), ('data', 1), ('that', 1), ('is', 1), ('coming', 1), ('from', 1), ('somewhere', 1), ('Streaming', 1), ('', 1), ('', 1), ('-', 1), ('continuous', 1), ('flow', 1), ('of', 1), ('data', 1), ('coming', 1), ('from', 1), ('somewhere', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69af7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_4 = rdd_3.reduceByKey(lambda a,b : a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552b13e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spark', 2), ('Open', 1), ('Source', 1), ('Analytical', 1), ('Engine', 1), ('Fault', 1), ('tolerant', 1), ('Persistence', 1), ('In-Memory', 1), ('using', 1), ('PySpark', 1), ('in', 1), ('Python', 1), ('', 7), ('Stream', 1), ('continuous', 2), ('of', 3), ('Buffering', 1), ('chunk', 1), ('is', 1), ('-', 10), ('Processing', 1), ('Immutable', 1), ('Cache', 1), ('&', 1), ('Real', 1), ('time', 1), ('processing', 1), ('Streaming', 2), ('and', 1), ('Kafka', 1), ('Library', 1), ('written', 1), ('flow', 2), ('data', 3), ('that', 1), ('coming', 2), ('from', 2), ('somewhere', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91684492",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_5 = rdd_4.map(lambda x : (x[1], x[0])).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1651e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Open'), (1, 'Source'), (1, 'Analytical'), (1, 'Engine'), (1, 'Fault'), (1, 'tolerant'), (1, 'Persistence'), (1, 'In-Memory'), (1, 'using'), (1, 'PySpark'), (1, 'in'), (1, 'Python'), (1, 'Stream'), (1, 'Buffering'), (1, 'chunk'), (1, 'is'), (1, 'Processing'), (1, 'Immutable'), (1, 'Cache'), (1, '&'), (1, 'Real'), (1, 'time'), (1, 'processing'), (1, 'and'), (1, 'Kafka'), (1, 'Library'), (1, 'written'), (1, 'that'), (2, 'Spark'), (2, 'continuous'), (2, 'Streaming'), (2, 'flow'), (2, 'coming'), (2, 'from'), (2, 'somewhere'), (3, 'of'), (3, 'data'), (7, ''), (10, '-')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "475a9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_6 = rdd_4.filter(lambda x : 'S' in x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e194450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spark', 2), ('Source', 1), ('PySpark', 1), ('Stream', 1), ('Streaming', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a325df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b55ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ab5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208dde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610b4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"John\", \"IT\", 45000),\n",
    "    (\"Max\", \"IT\", 50000),\n",
    "    (\"Shawn\", \"HR\", 35000),\n",
    "    (\"Nick\", \"HR\", 25000),\n",
    "    (\"Tom\", \"IT\", 75000),\n",
    "    (\"Matt\", \"IT\", 85000),\n",
    "    (\"Jeff\", \"HR\", 105000),\n",
    "    (\"Chris\", \"IT\", 40000),\n",
    "    (\"Tom\", \"IT\", 45000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923bee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local[x] - running in standalone mode, and here x is the CPU cores I want to utilize\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Example_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f3ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000018D662D7A30>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6104007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-69NKCP74.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Example_1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18d662d7a30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c2c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkNewSession = SparkSession.newSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634dde68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.session.SparkSession.newSession(self)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkNewSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f00d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa08f678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26713dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce8e9666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "207d4023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b994e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "|   _1| _2|    _3|\n",
      "+-----+---+------+\n",
      "| John| IT| 45000|\n",
      "|  Max| IT| 50000|\n",
      "|Shawn| HR| 35000|\n",
      "| Nick| HR| 25000|\n",
      "|  Tom| IT| 75000|\n",
      "| Matt| IT| 85000|\n",
      "| Jeff| HR|105000|\n",
      "|Chris| IT| 40000|\n",
      "|  Tom| IT| 45000|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57bb51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name', 'Department', 'Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cf91863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = rdd.toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f208fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Department: string, Salary: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02621272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+\n",
      "| Name|Department|Salary|\n",
      "+-----+----------+------+\n",
      "| John|        IT| 45000|\n",
      "|  Max|        IT| 50000|\n",
      "|Shawn|        HR| 35000|\n",
      "| Nick|        HR| 25000|\n",
      "|  Tom|        IT| 75000|\n",
      "| Matt|        IT| 85000|\n",
      "| Jeff|        HR|105000|\n",
      "|Chris|        IT| 40000|\n",
      "|  Tom|        IT| 45000|\n",
      "+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e9517c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "046b2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([i for i in range(1,101)], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26e9454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b5f05f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "712928b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = spark.sparkContext.textFile('pyspark_intro.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d35c2b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark_intro.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7cf627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22eb1f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " '- Open Source Analytical Processing Engine',\n",
       " '- Immutable',\n",
       " '- Fault tolerant',\n",
       " '- Cache & Persistence',\n",
       " '- In-Memory',\n",
       " '- Real time processing using Streaming and Kafka',\n",
       " 'PySpark - Spark Library written in Python',\n",
       " '',\n",
       " 'Stream',\n",
       " '- continuous flow of data',\n",
       " '',\n",
       " 'Buffering    - chunk of data that is coming from somewhere',\n",
       " 'Streaming   - continuous flow of data coming from somewhere']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd425a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = spark.sparkContext.wholeTextFiles('pyspark_intro.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5057246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/C:/Users/asus/NCU_PySpark/pyspark_intro.txt',\n",
       "  'Spark\\r\\n- Open Source Analytical Processing Engine\\r\\n- Immutable\\r\\n- Fault tolerant\\r\\n- Cache & Persistence\\r\\n- In-Memory\\r\\n- Real time processing using Streaming and Kafka\\r\\nPySpark - Spark Library written in Python\\r\\n\\r\\nStream\\r\\n- continuous flow of data\\r\\n\\r\\nBuffering    - chunk of data that is coming from somewhere\\r\\nStreaming   - continuous flow of data coming from somewhere')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6b0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
